import torch
import torch.nn as nn
from ConvBlock import ConvBlock, DynamicPooling, LinearPerformerAttention, LinearParameterizationKernel, FastKernelCompression, LinearBlockSparseAttention, OptimizedDilatedResidual, TokenMerging, LinearLocalAttention, LinearDynamicInceptionBlock


def test_conv_block():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ ConvBlock"""
    print("=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ConvBlock ===")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–µ—Å—Ç–∞
    batch_size = 2
    seq_len = 64
    dim = 256
    heads = 8
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = ConvBlock(dim=dim, heads=heads, dropout=0.1)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    x = torch.randn(batch_size, seq_len, dim)
    
    print(f"–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {x.shape}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º forward pass
    try:
        with torch.no_grad():
            output = model(x)
        print(f"‚úÖ ConvBlock —Ä–∞–±–æ—Ç–∞–µ—Ç! –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {output.shape}")
        print(f"   –í—Ö–æ–¥: {x.shape} -> –í—ã—Ö–æ–¥: {output.shape}")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ ConvBlock: {e}")
        return False


def test_individual_components():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    print("\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ===")
    
    batch_size = 2
    seq_len = 32
    dim = 128
    
    components = [
        ("DynamicPooling", DynamicPooling(output_size=16, mode='adaptive')),
        ("LinearPerformerAttention", LinearPerformerAttention(dim, heads=4, feature_dim=64)),
        ("LinearParameterizationKernel", LinearParameterizationKernel(dim, dim, feature_dim=32)),
        ("FastKernelCompression", FastKernelCompression(dim, reduction_ratio=4)),
        ("LinearBlockSparseAttention", LinearBlockSparseAttention(dim, block_size=16, heads=4, feature_dim=64)),
        ("OptimizedDilatedResidual", OptimizedDilatedResidual(dim, dilations=[1, 2, 4], dropout=0.1)),
        ("TokenMerging", TokenMerging(dim, reduction_ratio=2)),
        ("LinearLocalAttention", LinearLocalAttention(dim, window_size=5, heads=4, feature_dim=32)),
        ("LinearDynamicInceptionBlock", LinearDynamicInceptionBlock(dim, kernel_sizes=[1, 3], feature_dims=[32, 32]))
    ]
    
    success_count = 0
    
    for name, component in components:
        try:
            x = torch.randn(batch_size, seq_len, dim)
            with torch.no_grad():
                output = component(x)
            print(f"‚úÖ {name}: {x.shape} -> {output.shape}")
            success_count += 1
        except Exception as e:
            print(f"‚ùå {name}: –û—à–∏–±–∫–∞ - {e}")
    
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç: {success_count}/{len(components)} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    return success_count == len(components)


def test_gradient_flow():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Ç–æ–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
    print("\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Ç–æ–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ ===")
    
    batch_size = 1
    seq_len = 16
    dim = 64
    
    model = ConvBlock(dim=dim, heads=4, dropout=0.1)
    x = torch.randn(batch_size, seq_len, dim, requires_grad=True)
    
    try:
        output = model(x)
        loss = output.mean()
        loss.backward()
        
        print(f"‚úÖ –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω—ã!")
        print(f"   –ì—Ä–∞–¥–∏–µ–Ω—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {x.grad.shape}")
        print(f"   –ù–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞: {x.grad.norm().item():.6f}")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {e}")
        return False


def test_memory_usage():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    print("\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ ===")
    
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ GPU")
    else:
        device = torch.device('cpu')
        print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ CPU")
    
    batch_size = 4
    seq_len = 128
    dim = 512
    
    model = ConvBlock(dim=dim, heads=8, dropout=0.1).to(device)
    x = torch.randn(batch_size, seq_len, dim).to(device)
    
    try:
        # –û—á–∏—â–∞–µ–º –∫—ç—à GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
        
        with torch.no_grad():
            output = model(x)
        
        if torch.cuda.is_available():
            final_memory = torch.cuda.memory_allocated()
            memory_used = final_memory - initial_memory
            print(f"‚úÖ –ü–∞–º—è—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞: {memory_used / 1024**2:.2f} MB")
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ {device}")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏: {e}")
        return False


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üß™ –ù–∞—á–∏–Ω–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ConvBlock")
    print(f"PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
    print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–µ—Å—Ç—ã
    tests = [
        test_conv_block,
        test_individual_components,
        test_gradient_flow,
        test_memory_usage
    ]
    
    results = []
    for test in tests:
        try:
            result = test()
            results.append(result)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ {test.__name__}: {e}")
            results.append(False)
    
    # –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print("\n" + "="*50)
    print("üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("="*50)
    
    test_names = [
        "ConvBlock (–æ—Å–Ω–æ–≤–Ω–æ–π)",
        "–û—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã", 
        "–ü–æ—Ç–æ–∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"
    ]
    
    for i, (name, result) in enumerate(zip(test_names, results)):
        status = "‚úÖ –ü–†–û–ô–î–ï–ù" if result else "‚ùå –ü–†–û–í–ê–õ–ï–ù"
        print(f"{i+1}. {name}: {status}")
    
    passed = sum(results)
    total = len(results)
    
    print(f"\n–û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {passed}/{total} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ")
    
    if passed == total:
        print("üéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´! ConvBlock —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
    else:
        print("‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–¥.")
    
    return passed == total


if __name__ == "__main__":
    main()